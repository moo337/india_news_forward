#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ì¸ë„ ë‰´ìŠ¤ í¬ë¡¤ë§ í›„ í…”ë ˆê·¸ë¨ ì±„ë„ë¡œ ì „ì†¡í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import json
import asyncio
import logging
from dotenv import load_dotenv
from telegram import Bot
from news_crawler_updated import IndiaNewsCrawler
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.DEBUG,
    handlers=[
        logging.FileHandler('news_forwarder.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def format_article_message(article):
    """ê¸°ì‚¬ ì •ë³´ë¥¼ í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    view_count = article.get('views', 0)
    if view_count >= 1000000:
        view_count_str = f"{view_count/1000000:.1f}M"
    elif view_count >= 1000:
        view_count_str = f"{view_count/1000:.1f}K"
    else:
        view_count_str = f"{view_count}"

    category = article.get('category', '')
    category_text = f" [{category}]" if category else ""

    message = (
        f"ğŸ“° *{article['title']}*{category_text}\n\n"
        f"ğŸ‘ ì¡°íšŒìˆ˜: {view_count_str}\n"
    )
    
    # ë°œí–‰ì¼ì´ ìˆìœ¼ë©´ ì¶”ê°€
    if article.get('published_date'):
        message += f"ğŸ“… ë°œí–‰ì¼: {article['published_date']}\n"
    
    message += f"ğŸ”— ë§í¬: {article['url']}\n"
    message += f"ì¶œì²˜: {article['source']}"
    
    return message

def save_crawl_results(articles, filename=None):
    """í¬ë¡¤ë§ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if filename is None:
        now = datetime.now()
        filename = f"crawled_articles_{now.strftime('%Y%m%d_%H%M%S')}.json"
    
    # test_data ë””ë ‰í† ë¦¬ í™•ì¸ ë° ìƒì„±
    test_data_dir = os.path.join(os.getcwd(), 'test_data')
    os.makedirs(test_data_dir, exist_ok=True)
    
    # íŒŒì¼ ê²½ë¡œ ìƒì„±
    filepath = os.path.join(test_data_dir, filename)
    
    # JSON íŒŒì¼ë¡œ ì €ì¥
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    
    logger.info(f"í¬ë¡¤ë§ ê²°ê³¼ë¥¼ {filepath}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    return filepath

async def crawl_and_send_news():
    """ë‰´ìŠ¤ í¬ë¡¤ë§ í›„ í…”ë ˆê·¸ë¨ ì±„ë„ë¡œ ì „ì†¡"""
    # Telegram ë´‡ í† í°ê³¼ ì±„ë„ ID ê°€ì ¸ì˜¤ê¸°
    bot_token = os.getenv('TELEGRAM_BOT_TOKEN')
    chat_id = os.getenv('TELEGRAM_CHAT_ID')
    
    logger.debug(f"í™˜ê²½ ë³€ìˆ˜ í™•ì¸ - ë´‡ í† í°: {'ì„¤ì •ë¨' if bot_token else 'ì—†ìŒ'}, ì±„ë„ ID: {chat_id if chat_id else 'ì—†ìŒ'}")
    
    if not bot_token or not chat_id:
        logger.error("í™˜ê²½ ë³€ìˆ˜ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (TELEGRAM_BOT_TOKEN, TELEGRAM_CHAT_ID)")
        return False
    
    try:
        # ë´‡ ì´ˆê¸°í™”
        bot = Bot(token=bot_token)
        logger.debug("í…”ë ˆê·¸ë¨ ë´‡ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        crawler = IndiaNewsCrawler()
        logger.debug("ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # í¬ë¡¤ë§ ì‹œì‘ ë©”ì‹œì§€ ì „ì†¡
        await bot.send_message(chat_id=chat_id, text="ê¸°ì‚¬ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”...")
        logger.debug("í¬ë¡¤ë§ ì‹œì‘ ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
        
        # Times of India í¬ë¡¤ë§
        logger.info("Times of India í¬ë¡¤ë§ ì‹œì‘")
        try:
            toi_articles = crawler.crawl_specific_website('times_of_india', max_articles=20)
            logger.debug(f"Times of India í¬ë¡¤ë§ ê²°ê³¼: {len(toi_articles)}ê°œ ê¸°ì‚¬")
            for i, article in enumerate(toi_articles):
                logger.debug(f"TOI ê¸°ì‚¬ {i+1}: {article.get('title')} - {article.get('published_date')}")
        except Exception as e:
            logger.error(f"Times of India í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            toi_articles = []
        
        await bot.send_message(
            chat_id=chat_id, 
            text=f"Times of Indiaì—ì„œ {len(toi_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤."
        )
        
        # Economic Times í¬ë¡¤ë§
        logger.info("Economic Times í¬ë¡¤ë§ ì‹œì‘")
        try:
            et_articles = crawler.crawl_specific_website('economic_times', max_articles=20)
            logger.debug(f"Economic Times í¬ë¡¤ë§ ê²°ê³¼: {len(et_articles)}ê°œ ê¸°ì‚¬")
            for i, article in enumerate(et_articles):
                logger.debug(f"ET ê¸°ì‚¬ {i+1}: {article.get('title')} - {article.get('published_date')}")
        except Exception as e:
            logger.error(f"Economic Times í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            et_articles = []
        
        await bot.send_message(
            chat_id=chat_id, 
            text=f"Economic Timesì—ì„œ {len(et_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤."
        )
        
        # ëª¨ë“  ê¸°ì‚¬ í†µí•©
        all_articles = toi_articles + et_articles
        logger.debug(f"ì´ {len(all_articles)}ê°œì˜ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥
        try:
            saved_file = save_crawl_results(all_articles)
            logger.debug(f"í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {saved_file}")
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        
        # ê²°ê³¼ ìš”ì•½ ë©”ì‹œì§€ ì „ì†¡
        summary = f"""
í¬ë¡¤ë§ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!

ì´ {len(all_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤:
- Times of India: {len(toi_articles)}ê°œ
- Economic Times: {len(et_articles)}ê°œ
        """
        await bot.send_message(chat_id=chat_id, text=summary)
        
        # ê° ê¸°ì‚¬ ê°œë³„ ì „ì†¡
        logger.info(f"ì´ {len(all_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì±„ë„ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.")
        for i, article in enumerate(all_articles, 1):
            message = format_article_message(article)
            try:
                logger.debug(f"ê¸°ì‚¬ {i}/{len(all_articles)} ì „ì†¡ ì‹œë„")
                await bot.send_message(chat_id=chat_id, text=message, parse_mode='Markdown')
                logger.debug(f"ê¸°ì‚¬ {i} ì „ì†¡ ì„±ê³µ")
                await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ {i} ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                try:
                    # Markdown íŒŒì‹± ì˜¤ë¥˜ ì‹œ ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì¬ì‹œë„
                    await bot.send_message(chat_id=chat_id, text=message)
                    logger.debug(f"ê¸°ì‚¬ {i} ì¼ë°˜ í…ìŠ¤íŠ¸ë¡œ ì¬ì „ì†¡ ì„±ê³µ")
                except Exception as e2:
                    logger.error(f"ê¸°ì‚¬ {i} ì¬ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e2)}")
                await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
        
        logger.info("ëª¨ë“  ê¸°ì‚¬ ì „ì†¡ ì™„ë£Œ")
        return True
        
    except Exception as e:
        logger.exception(f"ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        if 'bot' in locals() and 'chat_id' in locals():
            try:
                await bot.send_message(
                    chat_id=chat_id, 
                    text=f"ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì „ì†¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
                )
            except Exception as e2:
                logger.error(f"ì˜¤ë¥˜ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {str(e2)}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ì¸ë„ ë‰´ìŠ¤ ì „ì†¡ ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘")
    
    try:
        # Python 3.12 ì´ìƒì—ì„œëŠ” ìƒˆë¡œìš´ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„± ë°©ì‹ ì‚¬ìš©
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        result = loop.run_until_complete(crawl_and_send_news())
        loop.close()
        
        if result:
            logger.info("ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì „ì†¡ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            logger.error("ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ì „ì†¡ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.exception(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")
        raise

if __name__ == "__main__":
    main() 